{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST hand written digits exercise\n",
    "\n",
    "https://en.wikipedia.org/wiki/MNIST_database \n",
    "http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "In this example a neural network is trained to recognize handwritten digits.\n",
    "\n",
    "### about tensorflow\n",
    "\n",
    "Check this video for introduction to tensorflow concepts:\n",
    "\n",
    "https://www.youtube.com/watch?v=JO0LwmIlWw0&index=2&list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs\n",
    "\n",
    "\n",
    "### Exercise 1: \n",
    "\n",
    "Use the prev_validation_loss parameter and break out of the training epoch if the validation loss is higher than in previous round. This way we can avoid overfitting. Read about overfitting from the Internet what it actually means.\n",
    "\n",
    "### Exercise 2: \n",
    "\n",
    "Tune your model's hyperparameters so that you will get validation accuracy of 98%. \n",
    "\n",
    "  * you can change the activation functions (now sigmoid is used).\n",
    "  * you can change the hidden layer width.\n",
    "  * you can add more hidden layers (depth).\n",
    "  * you can modify the learning rate.\n",
    "  * what else?\n",
    "\n",
    "###  Exercise 3\n",
    "\n",
    "Calculate the test data accuracy. Test accuracy is usually calculated after the model is not changed anymore. The test accuracy is an estimate how well your model predicts for data that it has not seen before.\n",
    "\n",
    "You can read the test input and target data like this:\n",
    "\n",
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "\n",
    "### Exercise 4. \n",
    "\n",
    "Save the variables. Use tf.train.Saver(). Check detail from here: https://www.tensorflow.org/guide/saved_model\n",
    "\n",
    "### Exercise 5. \n",
    "\n",
    "Create a new Jupyter notebook. Copy the model without the training code to a the new notebook (you only need the output related parts (feedforward) to get the final output). Add code to load the saved variables. Test that loading works by calculating test data accuracy (same way as in exercise 3.\n",
    "\n",
    "### Exercise 6. \n",
    "\n",
    "Using paint or other drawing tool create a 28x28 image and draw a number there. Save the image and load it to the notebook created in exercise 5. Feed it to the model and check is the prediction correct.\n",
    "\n",
    "Use following package to load the image:\n",
    "\n",
    "import matplotlib.image as mimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = mimg.imread('image.png')\n",
    "\n",
    "plt.imshow(img) <-- use this to visualize how the image looks like.\n",
    "\n",
    "new_input = img[:,:,1] <--- we do not need RGB channels, just use one of the channels. \n",
    "\n",
    "new_input = new_input.reshape((1,28*28))  <-- you need to reshape to fit the input layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "TensorFLow includes a data provider for MNIST. This function automatically downloads the MNIST dataset to the chosen directory. The dataset is already split into training, validation, and test subsets. Furthermore, it preprocess it into a particularly simple and useful format. Every 28x28 image is flattened into a vector of length 28x28=784, where every value corresponds to the intensity of the color of the corresponding pixel. The samples are grayscale (but standardized from 0 to 1), so a value close to 0 is almost white and a value close to  is almost purely black. \n",
    "\n",
    "Since this is a classification problem, our targets are categorical. We must convert the categories using one-hot encoding. Each individual sample is converted to a vector of length 10 which has nine 0s and a single 1 at the position which corresponds to the correct answer. \n",
    "\n",
    "For instance, if the true answer is \"3\", the target will be [0,0,0,1,0,0,0,0,0,0] (counting from 0).\n",
    "\n",
    "if the true answer is \"9\", the target will be [0,0,0,0,0,0,0,0,0,1] (counting from 0).\n",
    "\n",
    "if the true answer is \"0\", the target will be [1,0,0,0,0,0,0,0,0,0] (counting from 0).\n",
    "\n",
    "if the true answer is \"7\", the target will be [1,0,0,0,0,0,0,1,0,0] (counting from 0).\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-319879c4c296>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\mickey\\AppData\\Local\\conda\\conda\\envs\\lecture_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\mickey\\AppData\\Local\\conda\\conda\\envs\\lecture_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\mickey\\AppData\\Local\\conda\\conda\\envs\\lecture_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\mickey\\AppData\\Local\\conda\\conda\\envs\\lecture_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\mickey\\AppData\\Local\\conda\\conda\\envs\\lecture_1\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_layer_size = 500\n",
    "output_size = 10\n",
    "\n",
    "# Reset any variables left in memory from previous runs. This needs to be run before changing tf variables.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declare placeholders where the data will be fed into.\n",
    "input_layer = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "weight_1 = tf.get_variable('weight_1',[input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable('biases_1', [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "# We've chosen Sigmoid as our activation function. You can try playing with different non-linearities. Relu will be better...\n",
    "output_1 = tf.nn.relu(tf.matmul(input_layer, weight_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first hiden layer and the output layer\n",
    "\n",
    "# when adding new hidden layers add them here.\n",
    "weight_2 = tf.get_variable('weight_2',[hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable('biases_2', [hidden_layer_size])\n",
    "output_2 = tf.nn.relu(tf.matmul(output_1, weight_2) + biases_2)\n",
    "\n",
    "# add 3 more hidden layers\n",
    "weight_3 = tf.get_variable('weight_3',[hidden_layer_size, hidden_layer_size])\n",
    "biases_3 = tf.get_variable('biases_3', [hidden_layer_size])\n",
    "output_3 = tf.nn.relu(tf.matmul(output_2, weight_3) + biases_3)\n",
    "\n",
    "weight_4 = tf.get_variable('weight_4',[hidden_layer_size, hidden_layer_size])\n",
    "biases_4 = tf.get_variable('biases_4', [hidden_layer_size])\n",
    "output_4 = tf.nn.relu(tf.matmul(output_3, weight_4) + biases_4)\n",
    "\n",
    "weight_5 = tf.get_variable('weight_5',[hidden_layer_size, output_size])\n",
    "biases_5 = tf.get_variable('biases_5', [output_size])\n",
    "\n",
    "# notice that we are not using an activation function for the last layer.\n",
    "# the output is the preciction of our network. \n",
    "# We could use softmax activaction function here. Check next comments for reasons why not.\n",
    "output = tf.matmul(output_4, weight_5) + biases_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss function for every output/target pair.\n",
    "# The function used is the same as applying softmax to the last layer and then calculating cross entropy\n",
    "# This function, however, combines them in a clever way, \n",
    "# which makes it both faster and more numerically stable (when dealing with very small numbers).\n",
    "# Logits here means: unscaled probabilities (so, the outputs, before they are scaled by the softmax)\n",
    "# Naturally, the labels are the targets.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=targets, logits=output)\n",
    "\n",
    "# Get the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Define the optimization step. \n",
    "# Using adaptive optimizers such as Adam in TensorFlow. This is the optimization algorithm.\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "# Check the tf.argmax documentation how this works. You can also experiment by printing the values\n",
    "# to visualize what is going on\n",
    "out_equals_targets = tf.equal(tf.argmax(output,1), tf.argmax(targets,1))\n",
    "\n",
    "# Accuracy of our model. Basically correct answers divided by the number of all answers.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_targets,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is ready\n",
    "\n",
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the session variable. In tensorflow all computations are done in a session. \n",
    "# In the jupyter notebook the InteractiveSession is easy to use...\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variables. Default initializer is Xavier.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Exercise 4 solution.\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# init is run in session. Otherwise nothing happens...\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc round: 0 training loss: 0.0169116107925001 validation loss: 0.078277946 validation_accuracy: 0.9822\n",
      "training ready\n"
     ]
    }
   ],
   "source": [
    "# Define the batch size (in what kind of chunks of data is feed to the model).\n",
    "# In many cases we cannot all data at once. There might be too much data to fit in memory.\n",
    "# For that reason we are dividing the data in batches.\n",
    "batch_size = 150\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "# epoch is one round of training with all the training data.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# How many epochs we want to train.\n",
    "max_epocs = 50\n",
    "\n",
    "# Keep track of the validation loss of the previous epoch.\n",
    "# If the validation loss becomes increasing, we want to trigger early stopping.\n",
    "# We initially set it at some arbitrarily high number to make sure we don't trigger it\n",
    "# at the first epoch\n",
    "prev_validation_loss = 999999.\n",
    "\n",
    "# Create a loop for the epochs. Epoch_counter is a variable which automatically starts from 0.\n",
    "for epoch_counter in range(max_epocs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range (batches_number):\n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)        \n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimizer, mean_loss],\n",
    "                                feed_dict={input_layer: input_batch, targets:target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "        \n",
    "    # So far curr_epoch_loss contained the sum of all batches inside the epoch\n",
    "    # We want to find the average batch losses over the whole epoch\n",
    "    # The average batch loss is a good proxy for the current epoch loss    \n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # At the end of each epoch, get the validation loss and accuracy\n",
    "    # Get the input batch and the target batch from the validation dataset    \n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "        \n",
    "    validation_accuracy, validation_loss = sess.run([accuracy, mean_loss],\n",
    "                                feed_dict={input_layer: input_batch, targets:target_batch})\n",
    "    \n",
    "    \n",
    "    # Exercise 1 solution.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "    else:\n",
    "        # Exercise 4 solution.\n",
    "        # Save the variables to disk.\n",
    "        saver.save(sess, \"./tmp/model.ckpt\")\n",
    "        \n",
    "    prev_validation_loss = curr_epoch_loss\n",
    "\n",
    "    # print result each epoch... As an exercise, you can try to pretty print the float values. \n",
    "    # For example only 3 decimals is enough.\n",
    "    print('epoc round: ' + str(epoch_counter)+\n",
    "         ' training loss: ' + str(curr_epoch_loss) +\n",
    "         ' validation loss: ' + str(validation_loss) +\n",
    "         ' validation_accuracy: ' + str(validation_accuracy))\n",
    "        \n",
    "    \n",
    "print('training ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3 solution:\n",
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "\n",
    "test_accuracy = sess.run(accuracy,\n",
    "                         feed_dict={input_layer: input_batch, targets:target_batch})\n",
    "\n",
    "print('test accuracy: ' + str(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
