{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In this notebook we will learn the basics of unsupervised learning. \n",
    "\n",
    "After completing this notebook you will be familiar with the basics of K-Means clustering as well as Principal Component Analysis for dimensionality reduction.\n",
    "\n",
    "- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance\n",
    "- Clustering looks to find homogeneous subgroups among the observations\n",
    "\n",
    "In concrete terms we will analyse a data set consisting of a bag of [stop words](https://en.wikipedia.org/wiki/Stop_words) from a collection of books. Our task is to try to group and identify the authors of the books.\n",
    "\n",
    "**Link to the data set source:** https://www.openml.org/d/458\n",
    "\n",
    "**Structure of the Notebook:**\n",
    "1. Exploring the data\n",
    " - Prepare the data for clustering\n",
    "1. K-Means clustering\n",
    " - Determine amount of clusters using the elbow method\n",
    " - Apply K-Means clustering to identify authors\n",
    " - Analyse the results\n",
    "1. Principal Component Analysis\n",
    " - Determine the amount of principal components\n",
    " - Analysing the principal components\n",
    " - Apply PCA to be used in clustering\n",
    " - Analyse the results\n",
    "1. Using PCA to visualize clusters in two dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library used for matrix and mathematical operations\n",
    "import pandas as pd # library used for data wrangling\n",
    "import matplotlib.pyplot as plt # library used for plotting data\n",
    "import seaborn as sns # a more sophisticated library for visualizations\n",
    "\n",
    "# display visualizations within the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "# a few personal preferences for the visualizations\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Use the `read_csv()` function from the Pandas library to load the dataset into a Pandas DataFrame. We'll call the variable `df` (short for dataframe).\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html#pandas.read_csv\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/api.html#dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (read csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "**Basic information about the dataset**\n",
    "\n",
    "Call the `info()` method on the newly created Pandas DataFrame.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html#pandas.DataFrame.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting a glimpse of the data**\n",
    "\n",
    "Call the `head()` method from the Pandas DataFrame to view the content of the first 5 rows of the DataFrame.\n",
    "\n",
    "**Tips:** You can return *n amount of rows by passing *n as a parameter value. You can list the last *n rows by calling the `tail()` method.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Books**\n",
    "\n",
    "Print the number of unique books by calling the `nunique()` function on the `BookID` column in the dataset.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/indexing.html\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.nunique.html#pandas.Series.nunique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (books summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**\n",
    "\n",
    "Print the number of authors by calling the `unique()` function on the `Author` column in the dataset.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.unique.html#pandas.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (author summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Books per author**\n",
    "\n",
    "Group the data by `Author` using the `groupby()` functiond. \n",
    "\n",
    "Print the number of books per author by calling the `nunique()` function on the the `BookID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (books per author summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Words**\n",
    "\n",
    "Print lists of top 10 words by:\n",
    "- Value count in an individual row\n",
    "- Sum or words occurring in total\n",
    "- Mean word occurence per row\n",
    "\n",
    "You can use `iloc` for slicing the dataframe (exclude last two columns) and `max()`, `sum()`, `mean()` functions in addition to a few familiar functions mentioned above to reach the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (max occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (sum of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (average word count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "K-Means might work in unintended ways if the unit and scale of data varies a lot. As we saw during the data analysis phase the data amount is rather homogenous. Nevertheless in this section we will standardize/normalize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract features and standardize the data**\n",
    "\n",
    "Scikit Learn provides a convenient `StandardScaler` class that you can import from the sklearn.preprocessing package. Create and instance of the class and call the `fit_transform()` method to standardize the data.  \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler\n",
    "\n",
    "Transform all data but the last column into a separate variable `X` using slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (standard scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering\n",
    "In K-means clustering, we seek to partition the observations into a pre-specified *K* number of distinct, non-overlapping clusters.\n",
    "\n",
    "**Documentation:** https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "**Theory/examples:** https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying number of clusters\n",
    "We can use the elbow method to determine the amount of clusters for the data. In the elbow method you will create 1 to *n* amount of clusters (where n is the amount of features) and store the Within Cluster Sum of Squares (WCSS) of each run. By plotting the WCSS values for each cluster amount we hope to see where the data converges.\n",
    "\n",
    "Import the `KMeans` class from the `sklearn.cluster` package and create a Python list called `wcss`.\n",
    "Loop through a range from 1 to num_features+1 and within this loop:\n",
    "- Apply KMeans with `i` number of clusters\n",
    "- Append the value from the `inertia_` attribute to the wcss list.\n",
    "\n",
    "Finally `plot()` the data using Matplotlib (and style the plot according to your liking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (elbow method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the  the actual K-Means clustering\n",
    "With this specific dataset we're mostly interested in clustering by author, which means that the number of clusters is 4.\n",
    "\n",
    "A few notes about the parameters of the `KMeas class`:\n",
    "- init is used to define centroid/cluster inintialization\n",
    "- n_init set the number of times the algorithm is run with different centroid seeds\n",
    "- max_iter sets the maximum number of iterations for a single run\n",
    "- random_state can be used if one needs reproducible results **NOTE:** Never use in production!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "**Tip:** You can both `fit()` and `predict()` the data at once calling the `fit_predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the results\n",
    "Now that we have divided the data into four distinct clusters it is time to evaluate the results. In general, evaluating the performance of clustering algorithms is challenging to say the least. In this dataset we have a target variable, which means that we can compare the output of our clustering algorithm to that. **Note:** In normal cases, where clustering is used, we do not have this novelty. Otherwise supervised learning is generally more accurate than unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the target/label to be used in metrics\n",
    "Before we can evalue the results the label/target variable needs to be converted to a numeric representation since strings (such as Shakespeare) cannot be numerically computed.\n",
    "\n",
    "Scikit Learn provides a convenient `LabelEncoder` class from the `sklearn.preprocessing` package to do this. Create an instance of the class and call the `fit_transform()` method to convert the labels into variable `y`.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (label encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homogeneity, completeness and V-measure\n",
    "Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis.\n",
    "\n",
    "In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:\n",
    "\n",
    "- **homogeneity:** each cluster contains only members of a single class.\n",
    "- **completeness:** all members of a given class are assigned to the same cluster.\n",
    "\n",
    "We can turn those concept as scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better).\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (homogeneity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harmonic mean from homogenity and completeness is called **V-measure** and is computed using the `v_measure_score()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (v measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance.\n",
    "\n",
    "**Documentation:** https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "**Theory/examples:** https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chosing number of Principal components\n",
    "In order to identify how much variance is explained by each Principal Component we will create an instance of th `PCA` class from the `sklearn.decomposition` package. Lastly `fit()` the data to the newly created object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the variance plots**\n",
    "\n",
    "We can plot the variance in two different ways. \n",
    "1. Plotting the `eplained_variance_ratio_` visualizes the percentage of variance each Principal Component explains. This allows us to locate a similar elbow curve as we did with the K-Means algorithm.\n",
    "1. Plotting the cumulative sum of the `eplained_variance_ratio_` visualizes the total amount of variance explained by a certain number of Principal Components combined. This might be useful if you are looking to reach a certain threshold level for the components. \n",
    "\n",
    "Create two separate plots visualizing both use cases. You can use th Numpy method `cumsum()` to calculate the cumulative sum of the components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (elbow method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (cumulative variance explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Principal Components\n",
    "Before we proceed with applying PCA in practice, let's have a look at how we can better understand the Principal Components.\n",
    "\n",
    "In order to more easily analyze the components we'll create a new Pandas DataFrame of the data:\n",
    "- Store the feature names in a variable named `columns` (remember to exclude the Author column)\n",
    "- Create a list of principal component identifiers i.e. \\['PC1', 'PC2', ... 'PCn'] (you can use Python [list comprehensions](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) for this\n",
    "- Store the principal components in a variable named `data` by accessing the `components_` attribute from the pca object\n",
    "- Create a new Pandas DataFrame using above variables (set the parameters `data`, `columns`, `index`). Call it `df_pca` so you do not overwrite the main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (dataframe from pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the data using `head()` to ensure that the DataFrame looks okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the Principal Components**\n",
    "\n",
    "A heatmap provides a good overview of the effect that each variable has on each principal component. We will utilize the `heatmap()` method from the Seaborn library to create the heatmap. For convenience we'll enlargen the plot by first calling the `figure()` method from the Matplotlib library and provide the figure dimensions as a tuple for the `figsize` parameter. E.g. use a size of *(25, 10)*.\n",
    "\n",
    "In addition, it might be a good idea to only plot the top 10 principal components to improve the readability of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (plot heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component 1\n",
    "The visualization might give an good overview but to more specifically understand the variables effect on the Principal Components we need to have a closer look at the data. \n",
    "\n",
    "Let's have a look at the variables that have the biggest positive and negative impact on the Principal component value.\n",
    "\n",
    "Slice the data using `loc` to only select the first principal component and use the `sort_values()` and `head()` functions to create the top lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (largest positive factors for PC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (largest negative factors for PC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the Pandas `describe()` method to get a statistical overview of the data.\n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (statistical description for PC1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat K-Means with Principal Components\n",
    "Now that we better understand PCA we can apply it to the clustering algorithm. \n",
    "Select two different amount of principal components:\n",
    "1. Number of components that explains at least 80% of the variance\n",
    "1. Number of components that explains at least 90% of the variance\n",
    "\n",
    "\n",
    "- Create the PCA object and apply the transformation (now you need to use `fit_transform()` to actually get the values for the principal components.\n",
    "- Create the K-Means object and predict the clusters.\n",
    "- Apply the three metrics used earlier and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA using number of components from the elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (homogeneity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (completeness score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (v measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA using ~80% variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (homogeneity score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (completeness score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (v measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PCA to Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (K-Means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (scatter plot, PC1 & PC2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (scatter plot, PC1 & PC3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (scatter plot, PC2 & PC3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison to actual labels**\n",
    "\n",
    "Finally we'll end the visualization by comparing the identified clusters and actual labels side by side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (scatter plot, cluster vs. labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONCLUSION/WHAT DID WE LEARN?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
